#Random Forest
set.seed(100)
rfGrid = expand.grid(mtry = c(8, 10, 15, 20, 30),
min.node.size = c(3, 5, 10, 20),
splitrule = "gini")
mod_rf_tune = train(overall_rating ~ . , data = train_pros_complete, method = "ranger",
num.trees = 50,
importance = "impurity",
tuneGrid = rfGrid,
trControl = trainControl("oob"))
rpart.plot(mod_rf_tune)
print(mod_rf_tune)
varImp(mod_rf_tune)
clean$cleaned_pros
trainDTM_pro_trim
View(trainDTM_pro_trim)
dim(trainDTM_pro_trim )
#Decision Trees
set.seed(100)
mod_tree = rpart(overall_rating ~ ., data = trainDTM_pro_trim, method = "class")
View(train_data)
#Decision Trees
trainDTM_DT = trainDTM_pro_trim %>%
as.matrix() %>%
as_tibble() %>%
mutate(overall_rating = train_data$overall_rating)
set.seed(100)
mod_tree = rpart(overall_rating ~ ., data = trainDTM_DT, method = "class")
rpart.plot(mod_tree)
rfGrid = expand.grid(mtry = c(8, 10, 15, 20, 30),
min.node.size = c(3, 5, 10, 20),
splitrule = "gini")
mod_rf_tune = train(overall_rating ~ . , data = trainDTM_DT, method = "ranger",
num.trees = 50,
importance = "impurity",
tuneGrid = rfGrid,
trControl = trainControl("oob"))
#Decision Trees
trainDTM_DT = trainDTM_pro_trim %>%
as.matrix() %>%
as_tibble() %>%
mutate(overall_rating = as.factor(train_data$overall_rating))
set.seed(100)
mod_tree = rpart(overall_rating ~ ., data = trainDTM_DT, method = "class")
rpart.plot(mod_tree)
#Random Forest
set.seed(100)
rfGrid = expand.grid(mtry = c(8, 10, 15, 20, 30),
min.node.size = c(3, 5, 10, 20),
splitrule = "gini")
mod_rf_tune = train(overall_rating ~ . , data = trainDTM_DT, method = "ranger",
num.trees = 50,
importance = "impurity",
tuneGrid = rfGrid,
trControl = trainControl("oob"))
varImp(mod_rf_tune)
set.seed(100)
mod_tree = rpart(overall_rating ~ ., data = trainDTM_DT, method = "class")
rpart.plot(mod_tree)
mod_rf_pros_varimp = train(overall_rating ~ . , data = trainDTM_DT, method = "ranger",
num.trees = 50,
importance = "impurity",
tuneGrid = rfGrid,
trControl = trainControl("oob"))
varImp(mod_rf_pros_varimp)
trainDTM_DT_cons = trainDTM_con_trim %>%
as.matrix() %>%
as_tibble() %>%
mutate(overall_rating = as.factor(train_data$overall_rating))
mod_rf_cons_varimp = train(overall_rating ~ . , data = trainDTM_DT_cons, method = "ranger",
num.trees = 50,
importance = "impurity",
tuneGrid = rfGrid,
trControl = trainControl("oob"))
varImp(mod_rf_cons_varimp)
mod_rf_tune = train(overall_rating ~ . , data = train_pros_complete, method = "ranger",
num.trees = 50,
importance = "impurity",
tuneGrid = rfGrid,
trControl = trainControl("oob"))
View(mod_rf_tune)
mod_rf_tune$metric
mod_rf_tune$bestTune
knitr::opts_chunk$set(echo = TRUE)
library(keras) #For RNN LSTM
library(tensorflow)
library(tidyverse) # for manipulation with data
library(caret) # for machine learning, including KNN
library(rpart) # for training decision trees
library(rpart.plot) # for plotting decision trees
library(klaR) #for Naive Bayes
library(ranger) # for training random forest
library(tm) # for text mining
library(wordcloud) # for text visualization
library(text2vec) # for GloVe, but we will use it to create DTM
library(arules)
library(arulesViz)
library(RColorBrewer)
glassdoorreviews = read.csv("glassdoor_reviews_clean.csv", header=TRUE, sep=",",
na.strings = c( "", " ", "0" , "NA"))
set.seed(100)
clean <- glassdoorreviews %>%
mutate(cleaned_headline = tolower(headline))%>%
mutate(cleaned_pros = tolower(pros))%>%
mutate(cleaned_cons = tolower(cons))%>%
mutate(cleaned_headline = gsub("[^a-z]", " ", cleaned_headline))%>%
mutate(cleaned_pros = gsub("[^a-z]", " ", cleaned_pros))%>%
mutate(cleaned_cons = gsub("[^a-z]", " ", cleaned_cons))%>%
dplyr::select(-headline, -pros, -cons)
clean = clean %>% dplyr::select(overall_rating, cleaned_pros, cleaned_cons)
sapply(clean, function(x) sum(is.na(x)))
clean = clean %>% na.omit()
clean = clean %>% sample_frac(0.06)
summary(clean)
clean = clean %>% mutate(overall_rating = case_when(overall_rating < 4 ~ 0,
overall_rating >= 4 ~ 1))
clean %>% group_by(overall_rating) %>%
summarise(N = n()) %>%
arrange(-N)
#Split whole dataset
p = 0.7
ind = runif(nrow(clean)) < p
train_data = clean[ind ,]
test_data = clean[!ind ,]
###Train and test DTM for pros
traincorpus_pro = VCorpus(VectorSource(train_data$cleaned_pros))
trainDTM_pro = DocumentTermMatrix(traincorpus_pro)
testcorpus_pro = VCorpus(VectorSource(test_data$cleaned_pros))
testDTM_pro = DocumentTermMatrix(testcorpus_pro)
cat("Dimensions of train DTM for pros =", dim(trainDTM_pro), "\n")
cat("Dimensions of test DTM for pros =", dim(testDTM_pro), "\n")
###Train and test DTM for cons
traincorpus_con = VCorpus(VectorSource(train_data$cleaned_cons))
trainDTM_con = DocumentTermMatrix(traincorpus_con)
testcorpus_con = VCorpus(VectorSource(test_data$cleaned_cons))
testDTM_con = DocumentTermMatrix(testcorpus_con)
cat("Dimensions of train DTM for cons=", dim(trainDTM_con), "\n")
cat("Dimensions of test DTM for cons=", dim(testDTM_con), "\n")
##########
#Trimming for pros
trainfreq_pros = termFreq(train_data$cleaned_pros)
trainfreq_words_pros = trainfreq_pros[trainfreq_pros>100]
trainfreq_words_pros = trainfreq_words_pros[!(names(trainfreq_words_pros) %in% stopwords())]
testfreq_pros = termFreq(test_data$cleaned_pros)
testfreq_words_pros = testfreq_pros[testfreq_pros>100]
testfreq_words_pros = testfreq_words_pros[!(names(testfreq_words_pros) %in% stopwords())]
trainDTM_pro_trim = trainDTM_pro[,colnames(trainDTM_pro) %in% names(trainfreq_words_pros)]
testDTM_pro_trim = testDTM_pro[,colnames(testDTM_pro) %in% names(trainfreq_words_pros)]
dim(trainDTM_pro_trim)
dim(testDTM_pro_trim)
#Trimming for cons
trainfreq_cons = termFreq(train_data$cleaned_cons)
trainfreq_words_cons = trainfreq_cons[trainfreq_cons>100]
trainfreq_words_cons = trainfreq_words_cons[!(names(trainfreq_words_cons) %in% stopwords())]
testfreq_cons = termFreq(test_data$cleaned_cons)
testfreq_words_cons = testfreq_cons[testfreq_cons>100]
testfreq_words_cons = testfreq_words_cons[!(names(testfreq_words_cons) %in% stopwords())]
trainDTM_con_trim = trainDTM_con[,colnames(trainDTM_con) %in% names(trainfreq_words_cons)]
testDTM_con_trim = testDTM_con[,colnames(testDTM_con) %in% names(trainfreq_words_cons)]
dim(trainDTM_con_trim)
dim(testDTM_con_trim)
#TCM for pros
token_pros = space_tokenizer(train_data$cleaned_pros)
it_pros = itoken(token_pros, progressbar = FALSE)
vocab_pro = create_vocabulary(it_pros) %>% prune_vocabulary(5)
vocab_pro_trim = vocab_pro[vocab_pro$term %in% names(trainfreq_words_pros), ]
vectorizer_pro = vocab_vectorizer(vocab_pro_trim)
tcm_pro = create_tcm(it_pros, vectorizer_pro, skip_grams_window = 3)
dim(tcm_pro)
#TCM for cons
token_cons = space_tokenizer(train_data$cleaned_cons)
it_cons = itoken(token_cons, progressbar = FALSE)
vocab_con = create_vocabulary(it_cons) %>% prune_vocabulary(5)
vocab_con_trim = vocab_con[vocab_con$term %in% names(trainfreq_words_cons), ]
vectorizer_con = vocab_vectorizer(vocab_con_trim)
tcm_con = create_tcm(it_cons, vectorizer_con, skip_grams_window = 3)
dim(tcm_con)
#Training GloVe for PROS
dim_word_emb = 30
glove_pros = GlobalVectors$new(rank = dim_word_emb, x_max = 10)
wv_main_pro = glove_pros$fit_transform(tcm_pro, n_iter = 50, convergence_tol = 0.001)
wv_context_pro = glove_pros$components
word_vectors_pro = wv_main_pro + t(wv_context_pro)
cat("Dim of word vector matrix =", dim(word_vectors_pro))
#Training GloVe for CONS
glove_cons = GlobalVectors$new(rank = dim_word_emb, x_max = 10)
wv_main_con = glove_cons$fit_transform(tcm_con, n_iter = 50, convergence_tol = 0.001)
wv_context_con = glove_cons$components
word_vectors_con = wv_main_con + t(wv_context_con)
cat("Dim of word vector matrix =", dim(word_vectors_con))
#PROS
dtm_glovevect_train_pros = as.matrix(trainDTM_pro_trim) %*% word_vectors_pro
dtm_glovevect_test_pros = as.matrix(testDTM_pro_trim) %*% word_vectors_pro
#CONS
dtm_glovevect_train_cons = as.matrix(trainDTM_con_trim) %*% word_vectors_con
dtm_glovevect_test_cons = as.matrix(testDTM_con_trim) %*% word_vectors_con
####Data preparation
ytrain = train_data %>% dplyr::select(overall_rating)
ytrain$overall_rating = factor(ytrain$overall_rating, ordered = TRUE, levels = c(0,1))
ytest = test_data %>% dplyr::select(overall_rating)
ytest$overall_rating = factor(ytest$overall_rating, ordered = TRUE, levels = c(0,1))
#Training data
x_train_pros = as.data.frame(dtm_glovevect_train_pros)
x_train_cons = as.data.frame(dtm_glovevect_train_cons)
train_complete = cbind(ytrain, Pros=x_train_pros, Cons=x_train_cons)
x_test_pros = as.data.frame(dtm_glovevect_test_pros)
x_test_cons = as.data.frame(dtm_glovevect_test_cons)
test_complete = cbind(ytest, Pros=x_test_pros, Cons=x_test_cons)
svm_linear = train(overall_rating ~ . , data = train_complete, method = "svmLinear", trControl = trainControl("none"))
svm_linear %>% predict(test_complete) %>% confusionMatrix(test_complete$overall_rating)
lstm_model = keras_model_sequential()
y_train_vect = matrix(as.numeric(train_complete$overall_rating)) -1
y_test_vect = matrix(as.numeric(test_complete$overall_rating)) -1
y_test_dummy = to_categorical(y_test_vect, num_classes = 2)
train_complete %>% dplyr::select(-overall_rating)
x_train_input = array(as.vector(t(train_complete %>% dplyr::select(-overall_rating))) ,dim=c(nrow(train_complete %>% dplyr::select(-overall_rating)) ,ncol(train_complete %>% dplyr::select(-overall_rating)) ,1))
x_test_input = array(as.vector(t(test_complete %>% dplyr::select(-overall_rating))) ,dim=c(nrow(test_complete %>% dplyr::select(-overall_rating)) ,ncol(test_complete %>% dplyr::select(-overall_rating)) ,1))
lstm_model = keras_model_sequential()
lstm_model %>%
layer_dense(units = 16, input_shape = c(965)) %>%
layer_activation('relu') %>%
layer_dense(units = 2) %>%
layer_activation('sigmoid')
lstm_model %>% compile (
loss = "categorical_crossentropy",
optimizer = "rmsprop",
metrics = c("accuracy")
)
summary(lstm_model)
lstm_model %>%
keras::fit(x_train_input ,y_train_dummy ,batch_size = 20 , epochs = 10)
###
y_train_vect = matrix(as.numeric(train_complete$overall_rating)) -1
y_train_dummy = to_categorical(y_train_vect, num_classes = 2)
y_test_vect = matrix(as.numeric(test_complete$overall_rating)) -1
y_test_dummy = to_categorical(y_test_vect, num_classes = 2)
x_train_input = array(as.vector(t(train_complete %>% dplyr::select(-overall_rating))) ,dim=c(nrow(train_complete %>% dplyr::select(-overall_rating)) ,ncol(train_complete %>% dplyr::select(-overall_rating)) ,1))
x_test_input = array(as.vector(t(test_complete %>% dplyr::select(-overall_rating))) ,dim=c(nrow(test_complete %>% dplyr::select(-overall_rating)) ,ncol(test_complete %>% dplyr::select(-overall_rating)) ,1))
lstm_model %>%
keras::fit(x_train_input ,y_train_dummy ,batch_size = 20 , epochs = 10)
lstm_model = keras_model_sequential()
lstm_model %>%
layer_dense(units = 16, input_shape = c(60,1)) %>%
layer_activation('relu') %>%
layer_dense(units = 2) %>%
layer_activation('sigmoid')
lstm_model %>% compile (
loss = "categorical_crossentropy",
optimizer = "rmsprop",
metrics = c("accuracy")
)
summary(lstm_model)
lstm_model %>%
keras::fit(x_train_input ,y_train_dummy ,batch_size = 20 , epochs = 10)
lstm_model = keras_model_sequential()
lstm_model %>%
layer_dense(units = 16, input_shape = c(60,2)) %>%
layer_activation('relu') %>%
layer_dense(units = 2) %>%
layer_activation('sigmoid')
lstm_model %>% compile (
loss = "categorical_crossentropy",
optimizer = "rmsprop",
metrics = c("accuracy")
)
summary(lstm_model)
lstm_model %>%
keras::fit(x_train_input ,y_train_dummy ,batch_size = 20 , epochs = 10)
lstm_model %>%
layer_dense(units = 16, input_shape = c(60,1)) %>%
layer_activation('relu') %>%
layer_dense(units = 2) %>%
layer_activation('sigmoid')
lstm_model %>% compile (
loss = "categorical_crossentropy",
optimizer = "rmsprop",
metrics = c("accuracy")
)
summary(lstm_model)
lstm_model = keras_model_sequential()
lstm_model %>%
layer_dense(units = 16, input_shape = c(60,1)) %>%
layer_activation('relu') %>%
layer_dense(units = 2) %>%
layer_activation('sigmoid')
lstm_model %>% compile (
loss = "categorical_crossentropy",
optimizer = "rmsprop",
metrics = c("accuracy")
)
summary(lstm_model)
lstm_model %>%
keras::fit(x_train_input ,y_train_dummy ,batch_size = 20 , epochs = 10)
lstm_model = keras_model_sequential()
lstm_model %>%
layer_dense(units = 16, input_shape = c(60,1)) %>%
layer_activation('relu') %>%
layer_dense(units = 2) %>%
layer_activation('sigmoid')
lstm_model %>% compile (
loss = "categorical_crossentropy",
optimizer = "rmsprop",
metrics = c("accuracy")
)
summary(lstm_model)
lstm_model %>%
keras::fit(x_train_input ,y_train_dummy ,batch_size = 24 , epochs = 10)
lstm_model = keras_model_sequential()
lstm_model %>%
layer_dense(units = 16, input_shape = c(60,)) %>%
layer_activation('relu') %>%
layer_dense(units = 2) %>%
layer_activation('sigmoid')
lstm_model = keras_model_sequential()
lstm_model %>%
layer_dense(units = 16, input_shape = c(60,2)) %>%
layer_activation('relu') %>%
layer_dense(units = 2) %>%
layer_activation('sigmoid')
lstm_model %>% compile (
loss = "categorical_crossentropy",
optimizer = "rmsprop",
metrics = c("accuracy")
)
summary(lstm_model)
lstm_model %>%
keras::fit(x_train_input ,y_train_dummy ,batch_size = 24 , epochs = 10)
lstm_model = keras_model_sequential()
lstm_model %>%
layer_dense(units = 16, input_shape = c(60,1)) %>%
layer_activation('relu') %>%
layer_dense(units = 2) %>%
layer_activation('sigmoid')
lstm_model %>% compile (
loss = "categorical_crossentropy",
optimizer = "rmsprop",
metrics = c("accuracy")
)
summary(lstm_model)
lstm_model %>%
keras::fit(x_train_input ,y_train_dummy ,batch_size = 24 , epochs = 10)
lstm_model = keras_model_sequential()
lstm_model %>%
layer_dense(units = 16, input_shape = c(60,1)) %>%
layer_activation('relu') %>%
layer_dense(units = 2, input_shape = c(60)) %>%
layer_activation('sigmoid')
lstm_model %>% compile (
loss = "categorical_crossentropy",
optimizer = "rmsprop",
metrics = c("accuracy")
)
summary(lstm_model)
lstm_model %>%
keras::fit(x_train_input ,y_train_dummy ,batch_size = 24 , epochs = 10)
lstm_model = keras_model_sequential()
lstm_model %>%
layer_dense(units = 16, input_shape = c(60,1)) %>%
layer_activation('relu') %>%
layer_dense(units = 2) %>%
layer_activation('sigmoid')
lstm_model %>% compile (
loss = "binary_crossentropy",
optimizer = "rmsprop",
metrics = c("accuracy")
)
summary(lstm_model)
lstm_model %>%
keras::fit(x_train_input ,y_train_dummy ,batch_size = 24 , epochs = 10)
lstm_model = keras_model_sequential()
lstm_model %>%
layer_dense(units = 16, input_shape = c(2)) %>%
layer_activation('relu') %>%
layer_dense(units = 2) %>%
layer_activation('sigmoid')
lstm_model %>% compile (
loss = "binary_crossentropy",
optimizer = "rmsprop",
metrics = c("accuracy")
)
summary(lstm_model)
lstm_model %>%
keras::fit(x_train_input ,y_train_dummy ,batch_size = 24 , epochs = 10)
lstm_model = keras_model_sequential()
lstm_model %>%
layer_dense(units = 16, input_shape = c(60,1)) %>%
layer_activation('relu') %>%
layer_dense(units = 2) %>%
layer_activation('sigmoid')
lstm_model %>% compile (
loss = "binary_crossentropy",
optimizer = "rmsprop",
metrics = c("accuracy")
)
summary(lstm_model)
lstm_model %>%
keras::fit(x_train_input ,y_train_dummy ,batch_size = 24 , epochs = 10)
lstm_model = keras_model_sequential()
lstm_model %>%
layer_dense(units = 16, input_shape = c(60,1)) %>%
layer_activation('relu') %>%
layer_dense(units = 2, input_shape = c(1)) %>%
layer_activation('sigmoid')
lstm_model %>% compile (
loss = "binary_crossentropy",
optimizer = "rmsprop",
metrics = c("accuracy")
)
summary(lstm_model)
lstm_model %>%
keras::fit(x_train_input ,y_train_dummy ,batch_size = 24 , epochs = 10)
lstm_model = keras_model_sequential()
lstm_model %>%
layer_dense(units = 16, input_shape = c(965,1)) %>%
layer_activation('relu') %>%
layer_dense(units = 2) %>%
layer_activation('sigmoid')
lstm_model %>% compile (
loss = "binary_crossentropy",
optimizer = "rmsprop",
metrics = c("accuracy")
)
summary(lstm_model)
lstm_model %>%
keras::fit(x_train_input ,y_train_dummy ,batch_size = 24 , epochs = 10)
lstm_model = keras_model_sequential()
lstm_model %>%
layer_dense(units = 16, input_shape = c(60)) %>%
layer_activation('relu') %>%
layer_dense(units = 2) %>%
layer_activation('sigmoid')
lstm_model %>% compile (
loss = "binary_crossentropy",
optimizer = "rmsprop",
metrics = c("accuracy")
)
summary(lstm_model)
lstm_model %>%
keras::fit(x_train_input ,y_train_dummy ,batch_size = 24 , epochs = 10)
#Random Forest
set.seed(100)
rfGrid = expand.grid(mtry = c(8, 10, 15, 20, 30),
min.node.size = c(3, 5, 10, 20),
splitrule = "gini")
#Random Forest
set.seed(100)
rfGrid = expand.grid(mtry = c(8, 10, 15, 20, 30),
min.node.size = c(3, 5, 10, 20),
splitrule = "gini")
mod_rf_tune = train(overall_rating ~ . , data = train_complete, method = "ranger",
num.trees = 50,
importance = "impurity",
tuneGrid = rfGrid,
trControl = trainControl("oob"))
mod_rf_tuned = train(overall_rating ~ . , data = train_pros_complete, method = "ranger",
num.trees = 50,
importance = "impurity",
tuneGrid = expand.grid(mod_rf_tune$bestTune),
trControl = trainControl("oob"))
actual_rf = data.frame(test_complete$overall_rating)
pred_rf = data.frame(mod_rf_tuned %>% predict(test_complete))
mod_rf_tuned
View(test_complete)
pred_rf = data.frame(mod_rf_tuned %>% predict(test_complete))
mod_rf_tuned = train(overall_rating ~ . , data = train_complete, method = "ranger",
num.trees = 50,
importance = "impurity",
tuneGrid = expand.grid(mod_rf_tune$bestTune),
trControl = trainControl("oob"))
actual_rf = data.frame(test_complete$overall_rating)
pred_rf = data.frame(mod_rf_tuned %>% predict(test_complete))
full_rf = cbind(actual_rf, pred_rf)
colnames(full_rf) = c("actual_rf", "pred_rf")
diff=0
for(i in 1:15011) {
if (full_rf[i,1] != full_rf[i,2]) {
diff = diff+1
} else {
diff = diff + 0
}
}
RMSE = sqrt(diff/15011)
RMSE
setwd("C:/Users/user/Desktop/SGH Assignment")
cov19=read,csv("Data.csv")
cov19=read.csv("Data.csv")
View(cov19)
colnames(cov19)=c('Country', 'Continent', 'Latitude', 'Longitude', 'Latitude': 'Lat',
'Avg Temp per year', 'Beds per thousand', 'Doctors per thousand', 'GDP per Capita',
'Population','Median Age','Percent Population Age over 65', 'Date', 'Daily tests',
'Cases', 'Deaths')
View(cov19)
colnames(cov19)=c('Country', 'Continent', 'Latitude', 'Longitude',
'Avg Temp per year', 'Beds per thousand', 'Doctors per thousand', 'GDP per Capita',
'Population','Median Age','Percent Population Age over 65', 'Date', 'Daily tests',
'Cases', 'Deaths')
View(cov19)
a = lm(Cases~'Avg Temp per year' + 'Beds per thousand' + 'Doctors per thousand' + 'GDP per Capita' +
'Population' + 'Median Age' + 'Percent Population Age over 65')
a = lm(Cases~'Avg Temp per year' + 'Beds per thousand' + 'Doctors per thousand' + 'GDP per Capita' +
'Population' + 'Median Age' + 'Percent Population Age over 65', data=cov19)
a = lm(Cases~Avg Temp per year + Beds per thousand + Doctors per thousand + GDP per Capita +
Population + Median Age + Percent Population Age over 65, data=cov19)
a = lm(Cases~Avg Temp per year + Beds per thousand + Doctors per thousand + GDP per Capita +Population + Median Age + Percent Population Age over 65, data=cov19)
colnames(cov19)=c('Country', 'Continent', 'Latitude', 'Longitude',
'Avg_Temp_per_year', 'Beds_per_thousand', 'Doctors_per_thousand', 'GDP_per_Capita',
'Population','Median_Age','Percent_Population_Age_over_65', 'Date', 'Daily_tests',
'Cases', 'Deaths')
a = lm(Cases~Avg_Temp_per_year + Beds_per_thousand + Doctors_per_thousand + GDP_per_Capita +Population + Median_Age + Percent_Population_Age_over_65, data=cov19)
View(a)
a
summary(a)
